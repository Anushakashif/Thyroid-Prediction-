# -*- coding: utf-8 -*-
"""Thyroid.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eqeBuKolCReiA0I-iq7Wo73e4PhNhS8M
"""

import pandas as pd
import seaborn as sns
df = pd.read_csv('/content/Thyroid_Diff.csv')
df

df.head()

df.info()

df.describe()

df.isnull().sum()

df.duplicated().sum()

df.drop_duplicates(inplace = True)

df.rename(columns={'Hx Radiothreapy': 'Hx Radiotherapy'}, inplace=True)

df.drop(columns=['Risk', 'Stage', 'Response'], inplace=True)

df['Recurred'].value_counts()

df['Recurred'].value_counts(normalize=True)

sns.pairplot(df)

df['Age'].unique()

df['Thyroid Function'].unique()

df['Physical Examination'].unique()

df['Adenopathy'].unique()

df['Adenopathy'].value_counts(normalize=True) * 100

df = df[~df['Adenopathy'].isin(['Extensive', 'Posterior'])]

df['Adenopathy'].value_counts(normalize=True) * 100

df['Pathology'].unique()

df['Focality'].unique()

from sklearn.preprocessing import LabelEncoder

# Select only object (categorical) columns
cat_cols = df.select_dtypes(include=['object']).columns

# Create a label encoder instance
le = LabelEncoder()

# Apply label encoding to each categorical column
df[cat_cols] = df[cat_cols].apply(lambda col: le.fit_transform(col))

import matplotlib.pyplot as plt

corr = df.corr()

plt.figure(figsize=(12,8))
sns.heatmap(corr, annot=True, fmt=".2f", cmap="coolwarm", linewidths=0.5)
plt.title("Correlation Matrix for Thyroid Recurrence Dataset")
plt.show()

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split

X = df.drop('Recurred', axis=1)
y = df['Recurred']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Apply SMOTE
smote = SMOTE(random_state=42)
X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)

print("Before SMOTE:", y_train.value_counts())
print("After SMOTE:", y_train_bal.value_counts())

from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

models = {
    "                   Logistic Regression": LogisticRegression(),
    "                  Gaussian Naive Bayes": GaussianNB(),
    "                   K-Nearest Neighbors": KNeighborsClassifier(),
    "                Support Vector Machine": SVC(),
    "                          RandomForest": RandomForestClassifier()
}

for name, model in models.items():
    model.fit(X_train, y_train)
    print(name + " trained.")

for name, model in models.items():
    print(name + ": {:.2f}%".format(model.score(X_test, y_test) * 100))

from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

for name, model in models.items():
    y_pred = model.predict(X_test)
    print(f"\n{name}")
    print(classification_report(y_test, y_pred))

    # Plot confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(5,4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix - {name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

import joblib

best_model = models["                          RandomForest"]   # your best model
joblib.dump(best_model, "thyroid_model.pkl")

print("Model saved as thyroid_model.pkl")

"""## This dataset contains 383 rows and 17 columns In this code, the thyroid recurrence dataset was first loaded and cleaned by checking for missing and duplicate values, with duplicates being removed to ensure data quality. The categorical variables were then label-encoded to convert them into numeric form suitable for machine learning models. Since the target variable (“Recurred”) was imbalanced, SMOTE (Synthetic Minority Over-sampling Technique) was applied to the training data to balance the class distribution. After preprocessing, five classification models—**Logistic Regression**, **Gaussian Naive Bayes**, **K-Nearest Neighbors (KNN)**, **Support Vector Machine (SVM)**, and **Random Forest**—were trained and evaluated on the test set using accuracy, classification reports, and confusion matrices. Among these models, **Random Forest** achieved the highest accuracy and performed best overall, as it effectively handled nonlinear relationships, feature interactions, and class imbalance within the dataset.

###
"""